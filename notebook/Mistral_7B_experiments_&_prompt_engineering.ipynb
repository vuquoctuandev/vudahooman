{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook show how to inference mistral 7B and implement some technique like rag or sth in colab enviroment"
      ],
      "metadata": {
        "id": "hwmQpitZF8AL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installtion requirements"
      ],
      "metadata": {
        "id": "zp4NlOiZHVSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip -q install bitsandbytes accelerate xformers einops"
      ],
      "metadata": {
        "id": "O1D07XZSwSwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mistral 7b base is predict next token that exactly what we learn about in LLM fundametal"
      ],
      "metadata": {
        "id": "24Jrj4fozKgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "bnb_4bit_use_double_quant=True,\n",
        "bnb_4bit_quant_type=\"nf4\",\n",
        "bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "model_id,\n",
        "trust_remote_code=True,\n",
        "quantization_config=bnb_config,\n",
        "device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "model_id,\n",
        ")"
      ],
      "metadata": {
        "id": "02cU7OQDUzwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"my dream is\"\n",
        "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_inputs = encodeds\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTWfYBD0wkEN",
        "outputId": "9e304917-b23b-4bf7-9c71-629c1756cf3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my dream is a dream of Dionysus' wine.357 So when you tell me that I am being intoxicated, do you think you can control this, you, an earthly animal, a mere man?358 But if you want to tell about a dream, let them tell about a dream.359 I have already told a dream in my speech.360 I have told how I dreamed that, as a newborn, I was thrown into the middle of the ocean and set down in a place full of savage Thracian dogs who tore me to pieces; and in a word, how I was saved.361 Now I ask in return: are you now able to tell in your speech a dream about the future and one about the past, or even one that will take place in the future and one in the present?\" Dionysus then answers, a bit annoyed with Ariadne: \"But you yourself\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next, Mistal 7b instruct is a model instruct fine-tuned from base model (like chat gpt or other chatbot)"
      ],
      "metadata": {
        "id": "oEMWgu-tz7Ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "model_id,\n",
        "trust_remote_code=True,\n",
        "quantization_config=bnb_config,\n",
        "device_map='auto',\n",
        ")"
      ],
      "metadata": {
        "id": "f9PhjDV_E5G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"[INST]hi, how was your day?[/INST]\"\n",
        "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_inputs = encodeds\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "tx4mnQ_-U4Ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6731ec90-8075-43b8-ced3-3da4b45c7552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1461: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]hi, how was your day?[/INST] Hello! My day was good, thank you for asking. I completed several tasks and assisted many users with their queries. How about you? How was your day?</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next, we going to implement some promt engineering technique\n"
      ],
      "metadata": {
        "id": "ucRZXvu12L4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(promt):\n",
        "  encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "  model_inputs = encodeds\n",
        "\n",
        "  generated_ids = model.generate(**model_inputs, max_new_tokens=200)\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  return decoded"
      ],
      "metadata": {
        "id": "VgXA1KRw_G8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one shot\n",
        "text = \"\"\"<s>[INST]Classify the text into neutral, negative or positive. [/INST]\n",
        "\n",
        "Text: I think the vacation is okay.</s>\n",
        "[INST]Sentiment:[/INST]\n",
        "\"\"\"\n",
        "print(gen(text)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_iTfHKf3Hv1",
        "outputId": "be7eb38e-f498-4915-f9d1-ad9aef1a9d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [INST]Classify the text into neutral, negative or positive. [/INST]\n",
            "\n",
            "Text: I think the vacation is okay.</s> \n",
            "[INST]Sentiment:[/INST]\n",
            "The sentiment of the text is neutral.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#few shot\n",
        "text=\"\"\"<s>[INST]A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:[/INST]\n",
        "We were traveling in Africa and we saw these very cute whatpus.</s>\n",
        "\n",
        "[INST]To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:[/INST]\"\"\"\n",
        "print(gen(text)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdddtTSc8To6",
        "outputId": "603f83e3-102e-467f-fe31-eeb724e9488e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [INST]A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:[/INST]\n",
            "We were traveling in Africa and we saw these very cute whatpus.</s> \n",
            " \n",
            "[INST]To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:[/INST]The kids were so excited to see the fireworks that they started farduddling around with excitement.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you go futher you may noctice the mistral 7B instruct model work worse than Chat GPT as I test in other prompt engineering techique.\n",
        "\n",
        "The main reason is Chat GPT is an end product.\n",
        "\n",
        "I'm going to experiment instrucion fine tuning in next notebook\n"
      ],
      "metadata": {
        "id": "eelIwcuS_cdm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7JZ5vbOk_yYr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}